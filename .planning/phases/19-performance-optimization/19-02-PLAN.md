# Plan: 19-02 File I/O Optimization & Indexing

## Objective

Optimize file I/O patterns with metadata caching and implement reverse index for keyword lookups to speed up context prediction.

## Execution Context

**Phase:** 19 - performance-optimization
**Plan:** 2 of 3
**Depends on:** 19-01 complete

**Key files:**
- `lib/context/context-analyzer.js` - Graph building (555 lines)
- `lib/context/context-predictor.js` - Prediction logic (623 lines)
- `lib/automation/dependency-analyzer.js` - Scanning logic (~350 lines)

## Context

### Current State (after 19-01)
- Performance metrics module available
- Global memory writes batched
- Command definitions cached
- Still using linear keyword searches
- File metadata re-computed on each analysis

### Performance Bottlenecks Addressed
1. **Keyword lookups** - Linear search O(n) through all keywords
2. **File metadata** - Re-scanned on each context analysis
3. **Relationship graph** - Rebuilt on each prediction request

### Target Outcomes
- Reverse index for O(1) keyword → files lookup
- Cached file metadata with invalidation
- Persistent relationship graph (load from cache)
- 2-5x faster map-codebase command

## Tasks

<task id="1" title="Implement file metadata cache with invalidation">
**Action:** Create metadata cache that persists file stats and invalidates on change

**Implementation:**
Create `lib/cache/metadata-cache.js`:
```javascript
const fs = require('fs');
const path = require('path');

class MetadataCache {
  constructor() {
    this.cache = new Map(); // path -> { mtime, size, keywords, imports }
    this.watchers = new Map();
    this.invalidationCount = 0;
  }

  get(filePath) {
    const cached = this.cache.get(filePath);
    if (!cached) return null;

    // Check if file changed
    try {
      const stats = fs.statSync(filePath);
      if (stats.mtimeMs !== cached.mtimeMs) {
        this.invalidate(filePath);
        return null;
      }
      return cached;
    } catch {
      this.invalidate(filePath);
      return null;
    }
  }

  set(filePath, metadata) {
    try {
      const stats = fs.statSync(filePath);
      this.cache.set(filePath, {
        ...metadata,
        mtimeMs: stats.mtimeMs,
        size: stats.size,
        cachedAt: Date.now()
      });
    } catch {
      // File doesn't exist, don't cache
    }
  }

  invalidate(filePath) {
    this.cache.delete(filePath);
    this.invalidationCount++;
  }

  invalidateDirectory(dirPath) {
    for (const key of this.cache.keys()) {
      if (key.startsWith(dirPath)) {
        this.invalidate(key);
      }
    }
  }

  getStats() {
    return {
      size: this.cache.size,
      invalidations: this.invalidationCount
    };
  }
}

module.exports = { MetadataCache, metadataCache: new MetadataCache() };
```

**Files:**
- Create: `lib/cache/metadata-cache.js`
- Modify: `lib/cache/index.js` to export

**Verification:**
- [ ] Metadata cached after first access
- [ ] Cache invalidated when file modified
- [ ] Stats show cache size and invalidations
</task>

<task id="2" title="Implement reverse keyword index">
**Action:** Build inverted index for keyword → files mapping

**Implementation:**
Create `lib/index/keyword-index.js`:
```javascript
class KeywordIndex {
  constructor() {
    this.index = new Map(); // keyword -> Set<filePath>
    this.fileKeywords = new Map(); // filePath -> Set<keyword>
    this.buildTime = null;
  }

  addFile(filePath, keywords) {
    // Remove old keywords for this file
    this.removeFile(filePath);

    // Add new keywords
    const keywordSet = new Set(keywords);
    this.fileKeywords.set(filePath, keywordSet);

    for (const keyword of keywords) {
      const lowerKeyword = keyword.toLowerCase();
      if (!this.index.has(lowerKeyword)) {
        this.index.set(lowerKeyword, new Set());
      }
      this.index.get(lowerKeyword).add(filePath);
    }
  }

  removeFile(filePath) {
    const oldKeywords = this.fileKeywords.get(filePath);
    if (oldKeywords) {
      for (const keyword of oldKeywords) {
        const files = this.index.get(keyword.toLowerCase());
        if (files) {
          files.delete(filePath);
          if (files.size === 0) {
            this.index.delete(keyword.toLowerCase());
          }
        }
      }
      this.fileKeywords.delete(filePath);
    }
  }

  search(keyword) {
    // O(1) lookup instead of O(n)
    return this.index.get(keyword.toLowerCase()) || new Set();
  }

  searchMultiple(keywords) {
    const results = new Map(); // filePath -> matchCount
    for (const keyword of keywords) {
      const files = this.search(keyword);
      for (const file of files) {
        results.set(file, (results.get(file) || 0) + 1);
      }
    }
    // Sort by match count
    return Array.from(results.entries())
      .sort((a, b) => b[1] - a[1])
      .map(([file, count]) => ({ file, matchCount: count }));
  }

  getStats() {
    return {
      uniqueKeywords: this.index.size,
      indexedFiles: this.fileKeywords.size,
      buildTime: this.buildTime
    };
  }
}

module.exports = { KeywordIndex, keywordIndex: new KeywordIndex() };
```

**Files:**
- Create: `lib/index/keyword-index.js`
- Create: `lib/index/index.js`

**Verification:**
- [ ] Single keyword lookup is O(1)
- [ ] Multi-keyword search returns ranked results
- [ ] Index updates when files added/removed
</task>

<task id="3" title="Integrate metadata cache into dependency analyzer">
**Action:** Modify dependency-analyzer.js to use metadata cache

**Implementation:**
```javascript
// In dependency-analyzer.js
const { metadataCache } = require('../cache/metadata-cache');
const { tracker } = require('../metrics/performance-tracker');

analyzeFile(filePath) {
  // Check cache first
  const cached = metadataCache.get(filePath);
  if (cached && cached.imports) {
    tracker.trackCacheHit('metadata', 0);
    return cached;
  }

  // Cache miss - do full analysis
  tracker.trackCacheMiss('metadata');
  const start = Date.now();

  const content = fs.readFileSync(filePath, 'utf8');
  tracker.trackFileRead(filePath, content.length, Date.now() - start);

  const analysis = {
    imports: this.extractImports(content),
    exports: this.extractExports(content),
    keywords: this.extractKeywords(filePath, content)
  };

  metadataCache.set(filePath, analysis);
  return analysis;
}
```

**Files:**
- Modify: `lib/automation/dependency-analyzer.js`

**Verification:**
- [ ] First analysis caches metadata
- [ ] Subsequent calls use cache
- [ ] Cache invalidation on file change works
</task>

<task id="4" title="Integrate keyword index into context predictor">
**Action:** Replace linear keyword search with index lookup

**Implementation:**
```javascript
// In context-predictor.js
const { keywordIndex } = require('../index/keyword-index');

// Replace findByKeyword linear search:
// OLD:
// for (const [file, keywords] of this.fileKeywords) {
//   if (keywords.some(k => k.includes(searchKeyword))) {
//     results.push(file);
//   }
// }

// NEW:
findByKeyword(searchKeyword) {
  return Array.from(keywordIndex.search(searchKeyword));
}

findByKeywords(keywords) {
  return keywordIndex.searchMultiple(keywords);
}

// Update buildIndex to populate keywordIndex:
buildIndex() {
  const start = Date.now();
  // ... existing file scanning
  for (const [file, keywords] of this.fileKeywords) {
    keywordIndex.addFile(file, Array.from(keywords));
  }
  keywordIndex.buildTime = Date.now() - start;
}
```

**Files:**
- Modify: `lib/context/context-predictor.js`

**Verification:**
- [ ] Keyword search uses index
- [ ] Performance improved for multi-keyword searches
- [ ] Index built during predictor initialization
</task>

<task id="5" title="Add persistence for relationship graph">
**Action:** Cache relationship graph to disk, reload on startup

**Implementation:**
Add to `lib/context/context-analyzer.js`:
```javascript
const GRAPH_CACHE_PATH = path.join(os.homedir(), '.gywd', 'cache', 'relationship-graph.json');

saveGraph() {
  const data = {
    version: 1,
    timestamp: Date.now(),
    relationships: Array.from(this.relationships.entries()),
    fileHashes: this.getFileHashes()
  };
  fs.mkdirSync(path.dirname(GRAPH_CACHE_PATH), { recursive: true });
  fs.writeFileSync(GRAPH_CACHE_PATH, JSON.stringify(data));
}

loadGraph() {
  try {
    const data = JSON.parse(fs.readFileSync(GRAPH_CACHE_PATH, 'utf8'));

    // Validate cache is fresh
    if (this.validateFileHashes(data.fileHashes)) {
      this.relationships = new Map(data.relationships);
      return true;
    }
    return false;
  } catch {
    return false;
  }
}

getFileHashes() {
  // Return map of file -> mtime for validation
}

validateFileHashes(hashes) {
  // Check if any tracked files have changed
}
```

**Files:**
- Modify: `lib/context/context-analyzer.js`

**Verification:**
- [ ] Graph saved after build
- [ ] Graph loaded on startup if valid
- [ ] Rebuild triggered if files changed
- [ ] 2-5x faster second startup
</task>

<task id="6" title="Add performance tests for optimizations">
**Action:** Create tests validating I/O optimizations

**Implementation:**
Add to `tests/performance/`:
```javascript
describe('I/O Optimizations', () => {
  describe('Metadata Cache', () => {
    test('repeated file analysis uses cache', () => {
      analyzer.analyzeFile(testFile);
      analyzer.analyzeFile(testFile);
      expect(tracker.metrics.cache.metadata.hits).toBe(1);
    });

    test('file modification invalidates cache', () => {
      analyzer.analyzeFile(testFile);
      fs.writeFileSync(testFile, 'modified');
      const result = metadataCache.get(testFile);
      expect(result).toBeNull();
    });
  });

  describe('Keyword Index', () => {
    test('search performance is O(1)', () => {
      // Build index with 1000 files
      const times = [];
      for (let i = 0; i < 100; i++) {
        const start = performance.now();
        keywordIndex.search('component');
        times.push(performance.now() - start);
      }
      const avgTime = times.reduce((a, b) => a + b) / times.length;
      expect(avgTime).toBeLessThan(1); // < 1ms
    });
  });

  describe('Relationship Graph Persistence', () => {
    test('cached graph loads in < 50ms', () => {
      analyzer.saveGraph();
      const start = Date.now();
      analyzer.loadGraph();
      expect(Date.now() - start).toBeLessThan(50);
    });
  });
});
```

**Files:**
- Create: `tests/performance/io-optimizations.test.js`

**Verification:**
- [ ] All optimization tests pass
- [ ] Performance improvements measurable
- [ ] No regressions in existing tests
</task>

## Verification

```bash
cd "C:\Users\Prithvi Putta\PMP-GYWD"

# Run all tests
npm test

# Run performance tests
npm test -- tests/performance/

# Measure map-codebase speed improvement
time npm run gywd -- map-codebase --dry-run
```

## Success Criteria

- [ ] New `lib/cache/metadata-cache.js` module
- [ ] New `lib/index/keyword-index.js` module
- [ ] Dependency analyzer uses metadata cache
- [ ] Context predictor uses keyword index
- [ ] Relationship graph persisted to disk
- [ ] O(1) keyword lookups verified
- [ ] All 557+ existing tests pass
- [ ] map-codebase 2x+ faster on warm cache

## Confidence Summary

**Overall: 82%** (High)

| Task | Confidence | Notes |
|------|------------|-------|
| T1: Metadata cache | 90% | Standard caching pattern |
| T2: Keyword index | 92% | Inverted index well-understood |
| T3: Integrate metadata cache | 80% | Multiple integration points |
| T4: Integrate keyword index | 78% | Requires careful refactoring |
| T5: Graph persistence | 75% | Cache invalidation complexity |
| T6: Performance tests | 85% | May need timing adjustments |

**Factors affecting confidence:**
- Graph persistence validation logic is complex
- Integration touches multiple modules
- Cache invalidation edge cases need careful handling
- Performance test timing can be flaky in CI

## Output

After execution, provide:
1. New cache/index modules created
2. Integration changes to existing modules
3. Before/after performance numbers
4. Cache hit rates observed
5. Any edge cases deferred

## Checkpoint

<checkpoint type="decision" after="task-3">
After integrating metadata cache, evaluate:
- Cache hit rate in real usage
- Memory overhead acceptable?
- Proceed with keyword index or optimize cache first?
</checkpoint>
